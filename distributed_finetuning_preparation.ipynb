{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### First get the candidate model parts (Head and Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:19:30.899306Z",
     "start_time": "2023-04-16T19:19:27.515339Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/16 19:19:28 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiffFeatureDetectorParamBiDirectional(\n",
       "  (cls_layer): Sequential(\n",
       "    (features_dropout): Dropout(p=0.31296189949335906, inplace=False)\n",
       "    (linear_0): Linear(in_features=1280, out_features=184, bias=True)\n",
       "    (relu_0): ReLU()\n",
       "    (hidden_dropout_0): Dropout(p=0.5838650129135917, inplace=False)\n",
       "    (linear_1): Linear(in_features=184, out_features=184, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "    (hidden_dropout_1): Dropout(p=0.5838650129135917, inplace=False)\n",
       "    (linear_2): Linear(in_features=184, out_features=184, bias=True)\n",
       "    (relu_2): ReLU()\n",
       "    (hidden_dropout_2): Dropout(p=0.5838650129135917, inplace=False)\n",
       "    (linear_out): Linear(in_features=184, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model_uri = \"models:/{}/{}\".format('Laion Balanced', '1')\n",
    "head_model = mlflow.pytorch.load_model(model_uri)\n",
    "head_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:29:33.174347Z",
     "end_time": "2023-04-16T22:30:03.030501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/16 22:29:37 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01c34ea6e5564a4ea39a02b0ec24b0a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "2023-04-16 22:30:01.330992: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-16 22:30:01.493154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 22:30:02.036493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from drecg.training.ignite_finetune import define_model_for_tune\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model_1 = define_model_for_tune('Laion Balanced', 'ViT_LAION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:37:18.240825Z",
     "start_time": "2023-04-16T19:37:18.228688Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.clip.modeling_clip.CLIPTextTransformer,\n",
       " transformers.models.clip.modeling_clip.CLIPVisionTransformer)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.feature_extractor.vit_model.text_model), type(model.feature_extractor.vit_model.vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.modeling_clip.CLIPModel"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPModel\n",
    "CLIPModel().push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:37:47.157400Z",
     "start_time": "2023-04-16T19:37:47.087454Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fced6f825e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_extractor.vit_model.text_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:39:34.936300Z",
     "start_time": "2023-04-16T19:39:34.931857Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_parameters(params):\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "text_params = count_parameters(model.feature_extractor.vit_model.text_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:39:31.999410Z",
     "start_time": "2023-04-16T19:39:31.932814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vision_params = count_parameters(model.feature_extractor.vit_model.vision_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:39:41.981678Z",
     "start_time": "2023-04-16T19:39:41.974196Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693021440"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:09.011644Z",
     "start_time": "2023-04-16T19:41:08.969576Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842777344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:28.979168Z",
     "start_time": "2023-04-16T19:41:28.972959Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes_per_param = 4\n",
    "vision_bytes = vision_params * bytes_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:38.561929Z",
     "start_time": "2023-04-16T19:41:38.517527Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vision_gigabytes = vision_bytes / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:40.430532Z",
     "start_time": "2023-04-16T19:41:40.425525Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.864880561828613"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_gigabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:49.172451Z",
     "start_time": "2023-04-16T19:41:49.163328Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_gigabytes = text_params * bytes_per_param / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:41:50.753259Z",
     "start_time": "2023-04-16T19:41:50.744320Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5817060470581055"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gigabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T21:14:25.292378Z",
     "end_time": "2023-04-16T21:14:26.244309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:15:30.807376Z",
     "end_time": "2023-04-16T21:15:49.113394Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6c7f425b98949dd81e3acf47b2a5727"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "vit_model = AutoModel.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.clip.modeling_clip.CLIPModel"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vit_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:16:11.289546Z",
     "end_time": "2023-04-16T21:16:11.336270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:17:10.649460Z",
     "end_time": "2023-04-16T21:17:10.708068Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "VitLaionFeatureExtractor(\n  (transforms): VitLaionPreProcess()\n  (vision_model): CLIPVisionTransformer(\n    (embeddings): CLIPVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n      (position_embedding): Embedding(257, 1664)\n    )\n    (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-47): 48 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n            (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n            (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n            (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n          )\n          (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n            (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n          )\n          (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n  )\n  (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:22:29.620003Z",
     "end_time": "2023-04-16T21:22:29.633958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers.models.clip.configuration_clip import CLIPVisionConfig\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "class VitImageFeatureExtractor(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = {\n",
    "          \"attention_dropout\": 0.0,\n",
    "          \"dropout\": 0.0,\n",
    "          \"hidden_act\": \"gelu\",\n",
    "          \"hidden_size\": 1664,\n",
    "          \"image_size\": 224,\n",
    "          \"initializer_factor\": 1.0,\n",
    "          \"initializer_range\": 0.02,\n",
    "          \"intermediate_size\": 8192,\n",
    "          \"layer_norm_eps\": 1e-05,\n",
    "          \"model_type\": \"clip_vision_model\",\n",
    "          \"num_attention_heads\": 16,\n",
    "          \"num_channels\": 3,\n",
    "          \"num_hidden_layers\": 48,\n",
    "          \"patch_size\": 14,\n",
    "          \"projection_dim\": 512,\n",
    "          \"transformers_version\": \"4.27.2\"\n",
    "        }\n",
    "        vision_config = CLIPVisionConfig(**config)\n",
    "\n",
    "        self.projection_dim = 1280\n",
    "        self.vision_embed_dim = 1664\n",
    "\n",
    "        self.vision_model = CLIPVisionTransformer(vision_config)\n",
    "        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.get_image_features(pixel_values=x)\n",
    "\n",
    "    def get_image_features(\n",
    "            self,\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "            output_hidden_states: Optional[bool] = False,\n",
    "            return_dict: Optional[bool] = True,\n",
    "        ) -> torch.FloatTensor:\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        return image_features\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:01.319643Z",
     "end_time": "2023-04-16T22:54:02.043658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "model_to_hub = VitImageFeatureExtractor()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:48:15.203876Z",
     "end_time": "2023-04-16T21:48:20.336115Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "model_to_hub.visual_projection = model.feature_extractor.visual_projection"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:48:24.501177Z",
     "end_time": "2023-04-16T21:48:24.507767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model_to_hub.vision_model = model.feature_extractor.vision_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:48:27.941987Z",
     "end_time": "2023-04-16T21:48:28.029125Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_to_hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel_to_hub\u001B[49m\u001B[38;5;241m.\u001B[39mpush_to_hub(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mViT-bigG-14-laion2B-39B-b160k\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_to_hub' is not defined"
     ]
    }
   ],
   "source": [
    "model_to_hub.push_to_hub(\"ViT-bigG-14-laion2B-39B-b160k\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:49:25.392117Z",
     "end_time": "2023-04-16T22:02:33.746603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub.\n"
     ]
    }
   ],
   "source": [
    "model = VitImageFeatureExtractor.from_pretrained(\"dbuos/ViT-bigG-14-laion2B-39B-b160k\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:08.505080Z",
     "end_time": "2023-04-16T22:54:17.242212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Extractor(torch.nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_a, img_b = x\n",
    "        return self.vision_model.get_image_features(pixel_values=img_a), self.vision_model.get_image_features(\n",
    "            pixel_values=img_b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:17.244380Z",
     "end_time": "2023-04-16T22:54:17.246192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model2 = Extractor(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:17.246334Z",
     "end_time": "2023-04-16T22:54:17.248813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Only model weights in GPU: 7.9 GB\n",
    "#Batch of 4 images pairs forward pass: +13.5GB\n",
    "#Batch of 4 images pairs backward pass: +5GB\n",
    "#Fordward pass time GPU batch 4 pairs: 300 ms\n",
    "#forward + Backward  pass time CPU batch 4 pairs: 13 segs + 40 segs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:41.951562Z",
     "end_time": "2023-04-16T22:54:41.997410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model2.to(device)\n",
    "model2.train()\n",
    "images_batch_a = torch.rand(4, 3, 224, 224).to(device)\n",
    "images_batch_b = torch.rand(4, 3, 224, 224).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:45.245951Z",
     "end_time": "2023-04-16T22:54:45.260293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=2560, out_features=1, bias=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layer = nn.Linear(1280*2, 1)\n",
    "out_layer.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:46.795535Z",
     "end_time": "2023-04-16T22:54:46.798348Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "loss = BCEWithLogitsLoss()\n",
    "y_true = torch.tensor([0.0, 1.1, 0.0, 1.0]).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:56.775524Z",
     "end_time": "2023-04-16T22:54:56.778074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:48:58.472966Z",
     "end_time": "2023-04-16T22:48:58.483173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13930.768013000488 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "out = model2((images_batch_a, images_batch_b))\n",
    "out = torch.cat(out, dim=1)\n",
    "out = out_layer(out)\n",
    "end = time.time()\n",
    "milliseconds = (end - start) * 1000\n",
    "print(f\"Time: {milliseconds} ms\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:55:03.371702Z",
     "end_time": "2023-04-16T22:55:17.304758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "out = out.squeeze()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:55:54.492972Z",
     "end_time": "2023-04-16T22:55:54.539825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "loss_val = loss(out, y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:55:54.967708Z",
     "end_time": "2023-04-16T22:55:54.980339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "loss_val.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:55:56.966460Z",
     "end_time": "2023-04-16T22:56:38.288666Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Clean cuda cache\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m out\n\u001B[1;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "#Clean cuda cache\n",
    "del out\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:51:32.934104Z",
     "end_time": "2023-04-16T22:51:32.936279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "CLIPVisionConfig {\n  \"attention_dropout\": 0.0,\n  \"dropout\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 1664,\n  \"image_size\": 224,\n  \"initializer_factor\": 1.0,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"layer_norm_eps\": 1e-05,\n  \"model_type\": \"clip_vision_model\",\n  \"num_attention_heads\": 16,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 48,\n  \"patch_size\": 14,\n  \"projection_dim\": 512,\n  \"transformers_version\": \"4.27.2\"\n}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = {\"attention_dropout\": 0.0,\n",
    "  \"dropout\": 0.0,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_size\": 1664,\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_factor\": 1.0,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 8192,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"model_type\": \"clip_vision_model\",\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_channels\": 3,\n",
    "  \"num_hidden_layers\": 48,\n",
    "  \"patch_size\": 14,\n",
    "  \"projection_dim\": 512,\n",
    "  \"transformers_version\": \"4.27.2\"}\n",
    "CLIPVisionConfig(**config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T21:38:37.623847Z",
     "end_time": "2023-04-16T21:38:37.714084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.clip.modeling_clip.CLIPEncoder"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.vision_model.encoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T23:03:20.520277Z",
     "end_time": "2023-04-16T23:03:20.527379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd0d7e94ccb776d367f1b5708497d31240ae7624b420edef6e7d497eefa657d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
