{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### First get the candidate model parts (Head and Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-20T14:04:27.798677Z",
     "end_time": "2023-04-20T14:04:33.757553Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/20 14:04:29 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": "DiffFeatureDetectorParamBiDirectional(\n  (cls_layer): Sequential(\n    (features_dropout): Dropout(p=0.31296189949335906, inplace=False)\n    (linear_0): Linear(in_features=1280, out_features=184, bias=True)\n    (relu_0): ReLU()\n    (hidden_dropout_0): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_1): Linear(in_features=184, out_features=184, bias=True)\n    (relu_1): ReLU()\n    (hidden_dropout_1): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_2): Linear(in_features=184, out_features=184, bias=True)\n    (relu_2): ReLU()\n    (hidden_dropout_2): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_out): Linear(in_features=184, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model_uri = \"models:/{}/{}\".format('Laion Balanced', '1')\n",
    "head_model = mlflow.pytorch.load_model(model_uri)\n",
    "head_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:29:33.174347Z",
     "end_time": "2023-04-16T22:30:03.030501Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/20 14:03:54 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47f942be19254ea29f44a0d68b1f4df6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b514010f5d134ef4ae7d109bd85a2b51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from drecg.training.ignite_finetune import define_model_for_tune\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model_1 = define_model_for_tune('Laion Balanced', 'ViT_LAION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from drecg.feature_extraction.distributed import VitImageFeatureExtractor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T14:14:34.598576Z",
     "end_time": "2023-04-20T14:14:35.453336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub.\n"
     ]
    }
   ],
   "source": [
    "model = VitImageFeatureExtractor.load_pretrained()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T14:14:36.062933Z",
     "end_time": "2023-04-20T14:14:46.563301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.clip.modeling_clip.CLIPVisionTransformer"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.vision_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:30:21.495648Z",
     "end_time": "2023-04-20T18:30:21.558147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPooling, BaseModelOutput\n",
    "import torch\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionConfig, CLIPEncoder\n",
    "from torch import nn\n",
    "\n",
    "class CLIPVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig, embeddings, pre_layrnorm):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.pre_layrnorm = pre_layrnorm\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self,pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        hidden_states = self.pre_layrnorm(hidden_states)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.post_layernorm(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "class CLIPEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n",
    "    [`CLIPEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: CLIPConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "\n",
    "\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            layer_outputs = encoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                causal_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        return (hidden_states,)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "class Extractor(torch.nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_a, img_b = x\n",
    "        return self.vision_model(img_a), self.vision_model(img_b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:17.244380Z",
     "end_time": "2023-04-16T22:54:17.246192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model2 = Extractor(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:17.246334Z",
     "end_time": "2023-04-16T22:54:17.248813Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Only model weights in GPU: 7.9 GB\n",
    "#Batch of 4 images pairs forward pass: +13.5GB\n",
    "#Batch of 4 images pairs backward pass: +5GB\n",
    "#Fordward pass time GPU batch 4 pairs: 300 ms\n",
    "#forward + Backward  pass time CPU batch 4 pairs: 13 segs + 40 segs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:54:41.951562Z",
     "end_time": "2023-04-16T22:54:41.997410Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd0d7e94ccb776d367f1b5708497d31240ae7624b420edef6e7d497eefa657d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
