{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction experiments\n",
    "This experiments are intended to test the performance of the model when using the feature extraction technique. The idea is to use the pre-trained model as a feature extractor and then train a new model using the extracted features. \n",
    "\n",
    "The features area extracted ahead of time and saved for performance reasons. The caveat is that we can not use the data augmentation techniques when extracting the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vit Feature extraction\n",
    "* Load the feature extractor model\n",
    "* Load the dataloaders for training and testing \n",
    "* Iterate and extract the features (save in a list of batches)\n",
    "* Check memory\n",
    "* Create a new Dataset with the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drecg.models.uform import get_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Laion Balanced'.\n",
      "2023/03/05 13:45:40 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: Laion Balanced, version 1\n",
      "Created version '1' of model 'Laion Balanced'.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ModelVersion: creation_timestamp=1678041940322, current_stage='None', description='', last_updated_timestamp=1678041940322, name='Laion Balanced', run_id='0d16bb7f58b543ae9c3f126cab327b2c', run_link='', source='./mlflow_artifacts/16/0d16bb7f58b543ae9c3f126cab327b2c/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "def promote_model_to_registry(run_id, artifact_name, model_name):\n",
    "    return mlflow.register_model(\n",
    "        f\"runs:/{run_id}/{artifact_name}\",\n",
    "        model_name\n",
    "    )\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "run_id = '0d16bb7f58b543ae9c3f126cab327b2c'\n",
    "artifact_name = 'model'\n",
    "promote_model_to_registry(run_id, artifact_name, 'Laion Balanced')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/16 19:17:55 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": "DiffFeatureDetectorParamBiDirectional(\n  (cls_layer): Sequential(\n    (features_dropout): Dropout(p=0.31296189949335906, inplace=False)\n    (linear_0): Linear(in_features=1280, out_features=184, bias=True)\n    (relu_0): ReLU()\n    (hidden_dropout_0): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_1): Linear(in_features=184, out_features=184, bias=True)\n    (relu_1): ReLU()\n    (hidden_dropout_1): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_2): Linear(in_features=184, out_features=184, bias=True)\n    (relu_2): ReLU()\n    (hidden_dropout_2): Dropout(p=0.5838650129135917, inplace=False)\n    (linear_out): Linear(in_features=184, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get mlflow model\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model_uri = \"models:/{}/{}\".format('Laion Balanced', '1')\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T19:17:55.459505Z",
     "end_time": "2023-04-16T19:17:58.993258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4a941b13bdb48ae8c7b23841064ff42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model('unum-cloud/uform-vl-english')\n",
    "# model = get_model('unum-cloud/uform-vl-multilingual')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9e55637b40b403484f1fd153d2d8922"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from drecg.feature_extraction.utils import VitLaionFeatureExtractor\n",
    "\n",
    "feat_extractor = VitLaionFeatureExtractor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from drecg.data.utils import create_dataloader_train\n",
    "\n",
    "dataloader_ref = create_dataloader_train(transforms=feat_extractor.transforms)\n",
    "dataloader_new = create_dataloader_train(transforms=model.preprocess_image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "sample_batch_ref = next(iter(dataloader_ref))\n",
    "sample_batch_new = next(iter(dataloader_new))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_ref, y_ref, paths_ref = sample_batch_ref\n",
    "imga_ref, imgb_ref = x_ref\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feat_extractor.eval()\n",
    "feat_extractor.to(device)\n",
    "imga_ref = imga_ref.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    feats = feat_extractor.vit_model.get_image_features(pixel_values=imga_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "x_new, y_new, paths_new = sample_batch_new\n",
    "imga_new, imgb_new = x_new\n",
    "imga_new = imga_new.to(device)\n",
    "with torch.no_grad():\n",
    "    image_features, image_embedding = model.encode_image(imga_new, return_features=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([32, 256]), torch.Size([32, 1280]), torch.float32, torch.float32)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding.shape, feats.shape, image_embedding.dtype, feats.dtype"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imga_ref: torch.Size([32, 3, 224, 224])\n",
      "imgb_ref: torch.Size([32, 3, 224, 224])\n",
      "y_ref: torch.Size([32])\n",
      "imga_new: torch.Size([32, 3, 224, 224])\n",
      "imgb_new: torch.Size([32, 3, 224, 224])\n",
      "y_new: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def validate_same_shape(batch_ref, batch_new):\n",
    "    x_ref, y_ref, paths_ref = batch_ref\n",
    "    imga_ref, imgb_ref = x_ref\n",
    "    x_new, y_new, paths_new = batch_new\n",
    "    imga_new, imgb_new = x_new\n",
    "    assert imga_ref.shape == imga_new.shape\n",
    "    assert imgb_ref.shape == imgb_new.shape\n",
    "    assert y_ref.shape == y_new.shape\n",
    "\n",
    "    ##Print shapes of both batches\n",
    "    print(f'imga_ref: {imga_ref.shape}')\n",
    "    print(f'imgb_ref: {imgb_ref.shape}')\n",
    "    print(f'y_ref: {y_ref.shape}')\n",
    "    print(f'imga_new: {imga_new.shape}')\n",
    "    print(f'imgb_new: {imgb_new.shape}')\n",
    "    print(f'y_new: {y_new.shape}')\n",
    "\n",
    "validate_same_shape(sample_batch_ref, sample_batch_new)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "isinstance(feat_extractor.transforms, torch.nn.Module)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from drecg.feature_extraction.utils import extract_features_with_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0d29ec6e8584ae89d6398cbbaaa6460"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cddda8a152543139f259ba2a22a56b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/122 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "345c1eefac7e4ecc954170dd21d7f5be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "567639d747ae43298ee428aaf924891d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/122 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a655982959854a72aa24edea8a9569aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/122 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "295a4ab19b9b4844a135c23a212edb70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory feat_extracted/features_ext_vit_laion does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mextract_features_with_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mViT_LAION\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeat_extracted/features_ext_vit_laion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/data/Projects/dog-recognition/drecg/feature_extraction/utils.py:86\u001B[0m, in \u001B[0;36mextract_features_with_model\u001B[0;34m(model, root_dir)\u001B[0m\n\u001B[1;32m     81\u001B[0m train_augmented_features \u001B[38;5;241m=\u001B[39m extract_features(train_augmented_dataloader, feat_extractor, data\u001B[38;5;241m=\u001B[39mtrain_features,\n\u001B[1;32m     82\u001B[0m                                             device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     83\u001B[0m train_augmented_features \u001B[38;5;241m=\u001B[39m extract_features(train_augmented_dataloader, feat_extractor,\n\u001B[1;32m     84\u001B[0m                                             data\u001B[38;5;241m=\u001B[39mtrain_augmented_features, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m---> 86\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mroot_dir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/test_features.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(train_features, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mroot_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/train_features.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     88\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(validation_features, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mroot_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/validation_features.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/mambaforge/envs/dogrec/lib/python3.10/site-packages/torch/serialization.py:422\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[1;32m    419\u001B[0m _check_dill_version(pickle_module)\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m--> 422\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_zipfile_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m    423\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001B[1;32m    424\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/dogrec/lib/python3.10/site-packages/torch/serialization.py:309\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[0;34m(name_or_buffer)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    308\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[0;32m--> 309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/dogrec/lib/python3.10/site-packages/torch/serialization.py:287\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_zipfile_writer_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPyTorchFileWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Parent directory feat_extracted/features_ext_vit_laion does not exist."
     ]
    }
   ],
   "source": [
    "extract_features_with_model(model='ViT_LAION', root_dir='feat_extracted/features_ext_vit_laion')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from drecg.data.utils import FeaturesDataset\n",
    "root_dir = 'features_ext_vit_2'\n",
    "train_dataset_augmented = FeaturesDataset(f'{root_dir}/train_features_augmented.pt')\n",
    "validation_dataset = FeaturesDataset(f'{root_dir}/validation_features.pt')\n",
    "test_dataset = FeaturesDataset(f'{root_dir}/test_features.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1024]),\n torch.Size([1024]),\n device(type='cpu'),\n torch.float32,\n device(type='cuda', index=0))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_a, img_b), y = train_dataset_augmented[0]\n",
    "img_a.shape, img_b.shape, img_a.device, img_b.dtype, y.device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1024]),\n torch.Size([1024]),\n torch.float32,\n torch.float32,\n tensor(0))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_a, img_b), y = train_dataset_augmented[0]\n",
    "img_a.shape, img_b.shape, img_a.dtype, img_b.dtype, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/22 20:20:02 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b6963a9df1c42369cf8b8372fc8c257"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "seed_everything done:  42\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1/121]   1%|           [00:00<?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "300234f8f80948cfa5a5e78d5e9208cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/dogrec/lib/python3.10/site-packages/ignite/contrib/handlers/base_logger.py:132: UserWarning: Provided metric name 'accuracy' is missing in engine's state metrics: []\n",
      "  warnings.warn(\n",
      "/home/daniel/mambaforge/envs/dogrec/lib/python3.10/site-packages/ignite/contrib/handlers/base_logger.py:132: UserWarning: Provided metric name 'loss' is missing in engine's state metrics: []\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1/121]   1%|           [00:00<?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2f8c4a23ae246f4a45b13fa40b902ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[1/121]   1%|           [00:00<?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b81a143f55e49309d671ade705d4c2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[1/121]   1%|           [00:00<?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "064e1c5dd7b34ce5a72ccd62d45f361f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[1/121]   1%|           [00:00<?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6c33c01b94f4b1d9f7ce0e871bc7cf6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/dogrec/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from drecg.training.ignite_finetune import train\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment('Uform Fine Tuning')\n",
    "with mlflow.start_run():\n",
    "    # train(5, model_head_name='Laion Balanced', feat_ext_name='ViT_LAION')\n",
    "    train(5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'11.7'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DiffFeatureDetectorParamBiDirectional(\n  (cls_layer): Sequential(\n    (features_dropout): Dropout(p=0.20600446496280106, inplace=False)\n    (linear_0): Linear(in_features=256, out_features=21, bias=True)\n    (relu_0): ReLU()\n    (hidden_dropout_0): Dropout(p=0.30559697271906344, inplace=False)\n    (linear_1): Linear(in_features=21, out_features=21, bias=True)\n    (relu_1): ReLU()\n    (hidden_dropout_1): Dropout(p=0.30559697271906344, inplace=False)\n    (linear_out): Linear(in_features=21, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = model.head\n",
    "head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "vit_model = model.feature_extractor.vit_model.img_encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "VisualEncoder(\n  (encoder): VisionTransformer(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      (norm): Identity()\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (norm_pre): Identity()\n    (blocks): Sequential(\n      (0): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (1): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (2): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (3): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (4): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (5): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (6): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (7): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (8): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (9): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (10): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n      (11): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): LayerScale()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): LayerScale()\n        (drop_path2): Identity()\n      )\n    )\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (fc_norm): Identity()\n    (head): Identity()\n  )\n  (proj): Linear(in_features=768, out_features=256, bias=False)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 3, 224, 224]), torch.Size([10, 3, 224, 224]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "v3 = torch.randn((10, 3, 224, 224))\n",
    "img_a_batch.shape, v3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-6.2998e-01,  2.5293e-01,  4.5888e-01,  ...,  9.6236e-02,\n           -6.7797e-01,  2.9705e-01],\n          [ 8.7818e-01,  7.4715e-01, -3.9827e-01,  ...,  2.0539e-01,\n            6.1649e-01,  1.3331e+00],\n          [ 7.0685e-01,  1.0762e+00, -3.3192e-01,  ..., -3.6140e-01,\n            6.3357e-01,  1.0098e+00],\n          ...,\n          [-1.3128e-01,  6.8886e-02, -3.4928e-02,  ..., -8.7357e-02,\n            9.9788e-02, -1.6247e-01],\n          [ 3.8275e-02,  4.6661e-01,  3.1559e-01,  ..., -3.2795e-01,\n            2.4437e-01,  6.2411e-01],\n          [ 5.1062e-02, -5.0209e-01, -1.5708e-01,  ...,  3.3960e-01,\n            5.6685e-01, -1.7917e-01]],\n \n         [[-7.0150e-01,  2.1580e-01,  4.3162e-01,  ...,  7.7260e-02,\n           -6.4393e-01,  3.2967e-01],\n          [ 1.4705e-01,  2.2726e-01,  2.6083e-02,  ...,  8.0055e-02,\n           -2.5484e-02,  5.0873e-02],\n          [ 9.7804e-01,  6.7927e-01, -6.4200e-02,  ...,  2.0482e-01,\n           -3.4817e-02,  5.4986e-01],\n          ...,\n          [ 2.2198e+00,  1.8825e-01,  9.7204e-02,  ...,  1.9554e-01,\n            4.1926e-01,  2.4756e-01],\n          [ 3.6313e-01,  1.1056e-01,  4.0083e-01,  ..., -4.3440e-02,\n            5.0007e-01,  2.6232e-01],\n          [ 2.7446e-02, -7.7433e-01, -2.9524e-01,  ...,  4.8487e-01,\n            2.4034e-01, -5.3691e-02]],\n \n         [[-6.1435e-01,  2.0636e-01,  4.7957e-01,  ...,  6.0274e-02,\n           -7.1063e-01,  3.1299e-01],\n          [ 3.9123e-01,  1.2194e-01,  1.1415e-01,  ...,  3.9160e-01,\n            4.0192e-01, -2.4312e-03],\n          [ 7.0834e-02,  9.2951e-02,  3.5365e-02,  ...,  1.7897e-02,\n            5.6389e-02, -6.2564e-02],\n          ...,\n          [ 1.1213e+00,  4.7193e-01,  1.0510e-01,  ...,  6.1062e-01,\n           -2.3784e-01, -2.4692e-01],\n          [ 5.3317e-01,  1.6870e-01, -3.1094e-01,  ...,  4.4505e-01,\n            2.0070e-01,  1.9994e-01],\n          [ 3.7890e-02, -4.7258e-01,  2.8055e-03,  ...,  4.9735e-01,\n            6.2416e-02, -4.7494e-01]],\n \n         ...,\n \n         [[-5.6406e-01,  2.2444e-01,  4.1686e-01,  ...,  4.2009e-02,\n           -7.0881e-01,  3.0927e-01],\n          [ 1.4439e+00,  1.5412e+00,  3.4595e-03,  ...,  3.6591e-02,\n            8.0064e-01,  1.1657e+00],\n          [ 1.0686e+00,  4.0087e-01, -5.2730e-01,  ...,  4.3909e-02,\n           -2.4910e-01,  8.1028e-02],\n          ...,\n          [ 3.2777e-01,  7.4956e-02, -2.2766e-02,  ..., -1.3148e-01,\n            3.1703e-01,  3.8117e-01],\n          [ 6.5793e-01,  3.9729e-01, -2.4881e-01,  ...,  2.2246e-02,\n            3.8231e-01,  8.1604e-01],\n          [ 8.2656e-02, -5.8252e-01, -1.6692e-01,  ...,  4.5169e-01,\n            3.2964e-01, -2.4962e-01]],\n \n         [[-6.4122e-01,  2.4224e-01,  3.6355e-01,  ..., -5.8710e-02,\n           -8.0346e-01,  3.6254e-01],\n          [ 6.1618e-01,  6.2830e-01, -4.8847e-01,  ...,  4.4331e-02,\n            5.8522e-01,  1.0116e+00],\n          [ 1.4677e+00,  1.5622e+00, -4.7997e-01,  ...,  2.9806e-01,\n            1.0038e-01,  6.4570e-01],\n          ...,\n          [-3.3411e-01, -5.4553e-01, -2.1164e-01,  ...,  7.7290e-02,\n            3.4551e-01, -1.4736e-01],\n          [ 5.7256e-01,  5.7859e-01, -3.3128e-01,  ...,  3.9191e-01,\n            5.4410e-04,  9.4074e-02],\n          [-4.6856e-01, -5.5977e-01, -7.8281e-02,  ...,  3.2804e-01,\n            1.7019e-01, -1.6280e-01]],\n \n         [[-5.6031e-01,  1.4912e-01,  3.5619e-01,  ...,  4.0828e-02,\n           -7.9294e-01,  2.6884e-01],\n          [ 1.5444e+00,  1.2801e+00,  7.5150e-02,  ...,  7.3791e-01,\n           -4.0290e-01,  1.3665e-01],\n          [ 6.5692e-01,  5.4741e-01, -4.0675e-01,  ...,  5.2688e-02,\n            4.7902e-01,  5.0434e-01],\n          ...,\n          [ 4.8664e-01,  4.9155e-01,  2.5820e-01,  ...,  1.0256e-01,\n            7.1352e-02,  3.0699e-01],\n          [ 3.4263e-01, -1.2358e-01,  2.8031e-01,  ...,  4.3560e-01,\n           -1.8753e-01, -2.6634e-01],\n          [-1.2962e-01, -4.8300e-01,  6.6405e-03,  ...,  6.1407e-01,\n            3.3580e-01, -2.5031e-01]]], device='cuda:0',\n        grad_fn=<NativeLayerNormBackward0>),\n tensor([[ 0.8691, -0.6918,  0.7559,  ..., -2.0863,  0.5073,  1.1187],\n         [ 0.8254, -0.7051,  0.4640,  ..., -2.0039,  0.3911,  1.1678],\n         [ 1.0056, -0.6904,  0.9046,  ..., -1.8728,  0.5690,  1.3105],\n         ...,\n         [ 0.9185, -0.8851,  0.7588,  ..., -1.9107,  0.4629,  1.1643],\n         [ 0.9024, -1.0817,  0.4355,  ..., -1.5691,  0.4644,  1.0588],\n         [ 1.0861, -0.8495,  0.7008,  ..., -1.8743,  0.6397,  1.1195]],\n        device='cuda:0', grad_fn=<MmBackward0>))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model.to('cuda')\n",
    "vit_model(v3.to('cuda'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6482],\n        [-4.0981],\n        [ 1.4429],\n        [-1.3412],\n        [-0.9219],\n        [ 1.2507],\n        [ 0.7375],\n        [-0.3058],\n        [-1.0294],\n        [ 1.0365]], device='cuda:0', grad_fn=<DivBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "v0 = torch.randn((10, 256)).to(device)\n",
    "v1 = torch.randn((10, 256)).to(device)\n",
    "head((v0, v1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 3, 224, 224]), torch.Size([4, 3, 224, 224]), torch.Size([4]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_a_batch.shape, img_b_batch.shape, label_batch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/22 19:13:14 WARNING mlflow.pytorch: Stored model version '1.13.1' does not match installed PyTorch version '2.0.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a9d94e089544495a956d87fe5480be0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "CompleteModelToTune(\n  (head): DiffFeatureDetectorParamBiDirectional(\n    (cls_layer): Sequential(\n      (features_dropout): Dropout(p=0.20600446496280106, inplace=False)\n      (linear_0): Linear(in_features=256, out_features=21, bias=True)\n      (relu_0): ReLU()\n      (hidden_dropout_0): Dropout(p=0.30559697271906344, inplace=False)\n      (linear_1): Linear(in_features=21, out_features=21, bias=True)\n      (relu_1): ReLU()\n      (hidden_dropout_1): Dropout(p=0.30559697271906344, inplace=False)\n      (linear_out): Linear(in_features=21, out_features=1, bias=True)\n    )\n  )\n  (feature_extractor): UFormV1FeatureExtractor(\n    (vit_model): VLM(\n      (img_encoder): VisualEncoder(\n        (encoder): VisionTransformer(\n          (patch_embed): PatchEmbed(\n            (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n            (norm): Identity()\n          )\n          (pos_drop): Dropout(p=0.0, inplace=False)\n          (norm_pre): Identity()\n          (blocks): Sequential(\n            (0): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (1): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (2): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (3): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (4): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (5): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (6): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (7): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (8): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (9): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (10): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n            (11): Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n              )\n              (ls1): LayerScale()\n              (drop_path1): Identity()\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU(approximate='none')\n                (drop1): Dropout(p=0.0, inplace=False)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (ls2): LayerScale()\n              (drop_path2): Identity()\n            )\n          )\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (fc_norm): Identity()\n          (head): Identity()\n        )\n        (proj): Linear(in_features=768, out_features=256, bias=False)\n      )\n      (text_encoder): TextEncoder(\n        (backbone): TextEncoderBackbone(\n          (unimodal_encoder): ModuleList(\n            (0-1): 2 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (multimodal_encoder): ModuleList(\n            (0-1): 2 x FusedTransformerLayer(\n              (self_attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (cross_attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (context_proj): Identity()\n        (proj): Linear(in_features=768, out_features=256, bias=False)\n        (clf_head): Linear(in_features=768, out_features=2, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from drecg.training.ignite_finetune import define_model_for_tune\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "model_head_name='uform-vl-english'\n",
    "feat_ext_name='UForm_V1'\n",
    "define_model_for_tune(model_head_name, feat_ext_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c29f59fd37a24a039d32fdfd57306666"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "2023-04-08 16:11:16.565159: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-08 16:11:16.734112: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 16:11:17.311489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from drecg.feature_extraction.utils import VitLaionFeatureExtractor, VitFeatureExtractorComplete\n",
    "ext = VitFeatureExtractorComplete()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:10:47.247536Z",
     "end_time": "2023-04-08T16:11:18.501808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from drecg.data.utils import create_dataloader_train\n",
    "train_dataloader = create_dataloader_train(transforms=ext.transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:35.181601Z",
     "end_time": "2023-04-08T16:20:35.223894Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "(imgs_a, imgs_b), labels, paths = next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:35.730448Z",
     "end_time": "2023-04-08T16:20:37.787404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ext.to(device);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:38.554241Z",
     "end_time": "2023-04-08T16:20:40.676859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "imgs_a = imgs_a.to(device);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:40.677568Z",
     "end_time": "2023-04-08T16:20:40.682306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:41.780873Z",
     "end_time": "2023-04-08T16:20:41.798787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 257, 1664])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = ext.forward_single(imgs_a[:1], output_attentions=True, output_hidden_states=True, use_return_dict=True)\n",
    "out.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:20:58.266227Z",
     "end_time": "2023-04-08T16:20:58.307730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "att = MultiheadAttention(embed_dim=1664, num_heads=8, dropout=0.1, batch_first=True, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:23:28.377568Z",
     "end_time": "2023-04-08T16:23:28.419766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 224, 224])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:24:07.023749Z",
     "end_time": "2023-04-08T16:24:07.067735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "out_attn = att(out, out, out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:24:52.639388Z",
     "end_time": "2023-04-08T16:24:52.679730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 257, 1664])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_attn[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:26:09.591815Z",
     "end_time": "2023-04-08T16:26:09.596013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 1664]), torch.Size([1, 257, 1664]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = out.last_hidden_state\n",
    "last_hidden_state[:, 0, :].shape, last_hidden_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:06:00.333289Z",
     "end_time": "2023-04-08T16:06:00.339185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.models.clip.modeling_clip.CLIPModel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T13:40:39.012736Z",
     "end_time": "2023-04-08T13:40:39.024878Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(Linear(in_features=1664, out_features=1280, bias=False), False, False)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext.vit_model.visual_projection, ext.vit_model.config.output_attentions, ext.vit_model.config.output_hidden_states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T13:40:23.582024Z",
     "end_time": "2023-04-08T13:40:23.646950Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.clip.modeling_clip.CLIPVisionTransformer"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ext.vit_model.vision_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T13:51:34.860319Z",
     "end_time": "2023-04-08T13:51:34.904001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformers.models.clip.modeling_clip.CLIPVisionTransformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 2, 1)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer\n",
    "\n",
    "CLIPVisionTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T16:02:34.782556Z",
     "end_time": "2023-04-08T16:02:34.785358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "278"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Create a large list\n",
    "large_list = list(range(1000000))\n",
    "\n",
    "# Explicitly free the memory occupied by the large list\n",
    "del large_list\n",
    "gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T10:38:01.156841Z",
     "end_time": "2023-04-09T10:38:01.167922Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "tensors = [torch.rand(5, 5) for _ in range(10)]  # replace this with your list of tensors\n",
    "file_path = \"tensors.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"w\") as f:\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        f.create_dataset(f\"tensor_{i}\", data=tensor.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:37:00.521196Z",
     "end_time": "2023-04-09T11:37:01.654107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "file_to_read = \"feat_extracted/laion_last_hidden/test_features.pt\"\n",
    "import torch\n",
    "features = torch.load(file_to_read)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:38:40.364142Z",
     "end_time": "2023-04-09T11:38:44.325964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:39:08.296595Z",
     "end_time": "2023-04-09T11:39:08.300578Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:41:31.925211Z",
     "end_time": "2023-04-09T11:41:31.978942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "data = features[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:42:34.876915Z",
     "end_time": "2023-04-09T11:42:34.924396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "(feature_tensors_a, feature_tensors_b), labels, (path_str_a, path_str_b) = data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:45:06.382583Z",
     "end_time": "2023-04-09T11:45:06.387511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 257, 1664]) torch.Size([32, 257, 1664]) torch.Size([32]) 32 32\n"
     ]
    }
   ],
   "source": [
    "print(feature_tensors_a.shape, feature_tensors_b.shape, labels.shape, len(path_str_a), len(path_str_b))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T11:46:15.062340Z",
     "end_time": "2023-04-09T11:46:15.066927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "\n",
    "file_path = \"data.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"w\") as f:\n",
    "    for i, ((feature_tensors_a, feature_tensors_b), labels, (path_str_a, path_str_b)) in enumerate(features):\n",
    "        grp = f.create_group(f\"entry_{i}\")\n",
    "        grp.create_dataset(\"features_a\", data=feature_tensors_a.numpy())\n",
    "        grp.create_dataset(\"features_b\", data=feature_tensors_b.numpy())\n",
    "        grp.create_dataset(\"labels\", data=labels)\n",
    "        grp.attrs[\"paths_a\"] = path_str_a\n",
    "        grp.attrs[\"paths_b\"] = path_str_b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:00:34.957938Z",
     "end_time": "2023-04-09T12:00:35.245525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 8 batches [00:00, 15.35 batches/s]\n"
     ]
    }
   ],
   "source": [
    "from drecg.data.utils import convert_to_hdf5\n",
    "convert_to_hdf5(source_files = [\"feat_extracted/laion_last_hidden/test_features.pt\"], dest_path = \"test_features3.hdf5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:22:08.460450Z",
     "end_time": "2023-04-09T12:22:08.999592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_path = \"feat_extracted/laion_last_hidden_hdf5/train_features_augmented.hdf5\"\n",
    "f = h5py.File(file_path, \"r\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T13:20:11.122158Z",
     "end_time": "2023-04-09T13:20:11.237324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ -0.6300168 ,   0.8605088 ,  -1.5693604 , ...,  -1.4104166 ,\n        -10.171236  ,  -1.9453514 ],\n       [ -2.2364345 ,   1.7495991 ,   0.44537437, ...,   2.1975954 ,\n          0.9050273 ,  -0.6109122 ],\n       [  3.2277853 ,   1.5823239 ,  -1.3100352 , ...,   1.2139174 ,\n         -2.297767  ,   0.7951103 ],\n       ...,\n       [ -2.1315188 ,  -1.0066409 ,  -2.001802  , ...,   0.5799053 ,\n         -1.596676  ,  -0.6917656 ],\n       [ -2.0988412 ,  -1.1044948 ,  -2.921171  , ...,   0.84670824,\n         -7.2343183 ,  -6.1011157 ],\n       [ -2.7840497 ,  -3.143204  ,  -2.949893  , ...,  -1.4124207 ,\n         -4.018466  ,  -0.37366414]], dtype=float32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['entry_140']['features_a'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T13:21:37.470933Z",
     "end_time": "2023-04-09T13:21:37.516445Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "entry = f[\"entry_0\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:03:41.334578Z",
     "end_time": "2023-04-09T12:03:41.376405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entry[\"features_a\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:05:02.622108Z",
     "end_time": "2023-04-09T12:05:02.664443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['/home/daniel/data_dogs/testing/same/1402b.png',\n       '/home/daniel/data_dogs/testing/same/1403b.png',\n       '/home/daniel/data_dogs/testing/same/1404b.png',\n       '/home/daniel/data_dogs/testing/same/1408b.png',\n       '/home/daniel/data_dogs/testing/same/1411b.png',\n       '/home/daniel/data_dogs/testing/same/1413b.png',\n       '/home/daniel/data_dogs/testing/same/1414b.png',\n       '/home/daniel/data_dogs/testing/same/1420b.png',\n       '/home/daniel/data_dogs/testing/same/1423b.png',\n       '/home/daniel/data_dogs/testing/same/1426b.png',\n       '/home/daniel/data_dogs/testing/same/1429b.png',\n       '/home/daniel/data_dogs/testing/same/1430b.png',\n       '/home/daniel/data_dogs/testing/same/150b.png',\n       '/home/daniel/data_dogs/testing/same/151b.png',\n       '/home/daniel/data_dogs/testing/same/153b.png',\n       '/home/daniel/data_dogs/testing/same/156b.png',\n       '/home/daniel/data_dogs/testing/same/157b.png',\n       '/home/daniel/data_dogs/testing/same/158b.png',\n       '/home/daniel/data_dogs/testing/same/161b.png',\n       '/home/daniel/data_dogs/testing/same/162b.png',\n       '/home/daniel/data_dogs/testing/same/163b.png',\n       '/home/daniel/data_dogs/testing/same/165b.png',\n       '/home/daniel/data_dogs/testing/same/168b.png',\n       '/home/daniel/data_dogs/testing/same/169b.png',\n       '/home/daniel/data_dogs/testing/same/173b.png',\n       '/home/daniel/data_dogs/testing/same/176b.png',\n       '/home/daniel/data_dogs/testing/same/177b.png',\n       '/home/daniel/data_dogs/testing/same/178b.png',\n       '/home/daniel/data_dogs/testing/same/179b.png',\n       '/home/daniel/data_dogs/testing/same/181b.png',\n       '/home/daniel/data_dogs/testing/same/182b.png',\n       '/home/daniel/data_dogs/testing/same/184b.png'], dtype=object)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry.attrs[\"paths_b\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:06:39.496537Z",
     "end_time": "2023-04-09T12:06:39.544400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import h5py\n",
    "def load_data(file_path=\"data.hdf5\"):\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        data_features = []\n",
    "        for key in f.keys():\n",
    "            entry = f[key]\n",
    "            feature_tensors_a = torch.tensor(entry[\"features_a\"][()])\n",
    "            feature_tensors_b = torch.tensor(entry[\"features_b\"][()])\n",
    "            labels = torch.tensor(entry[\"labels\"][()])\n",
    "            path_str_a = entry.attrs[\"paths_a\"]\n",
    "            path_str_b = entry.attrs[\"paths_b\"]\n",
    "            data_features.append(((feature_tensors_a, feature_tensors_b), labels, (path_str_a, path_str_b)))\n",
    "\n",
    "    return data_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:26:44.958805Z",
     "end_time": "2023-04-09T12:26:45.004391Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### HDF5 Dataset for loading hidden state features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "file_to_read = \"feat_extracted/laion_last_hidden_hdf5/train_features.hdf5\"\n",
    "\n",
    "hfd5_file = h5py.File(file_to_read, \"r\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T14:58:08.395041Z",
     "end_time": "2023-04-09T14:58:09.372580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "121"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k in hfd5_file.keys()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T14:59:53.035991Z",
     "end_time": "2023-04-09T14:59:53.087747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(<HDF5 dataset \"features_a\": shape (32, 257, 1664), type \"<f4\">,\n <HDF5 dataset \"features_a\": shape (26, 257, 1664), type \"<f4\">)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_key = \"entry_0\"\n",
    "last_key = f\"entry_{len(hfd5_file.keys()) - 1}\"\n",
    "hfd5_file[first_key][\"features_a\"], hfd5_file[last_key][\"features_a\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T15:02:48.339314Z",
     "end_time": "2023-04-09T15:02:48.380394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 26)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfd5_file[first_key][\"features_a\"].shape[0], hfd5_file[last_key][\"features_a\"].shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T15:04:19.731363Z",
     "end_time": "2023-04-09T15:04:19.737736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "3866"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def items_in_file(hf):\n",
    "    num_keys = len(hf.keys())\n",
    "    batch_size = hf[\"entry_0\"][\"features_a\"].shape[0]\n",
    "    last_batch_size = hf[f\"entry_{num_keys - 1}\"][\"features_a\"].shape[0]\n",
    "    return num_keys * batch_size - (batch_size - last_batch_size)\n",
    "\n",
    "items_in_file(hfd5_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T15:07:44.265205Z",
     "end_time": "2023-04-09T15:07:44.316365Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:17:00.455291Z",
     "end_time": "2023-04-12T16:17:01.067037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "features = torch.rand((32, 256, 1668))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:18:04.343671Z",
     "end_time": "2023-04-12T16:18:04.396064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 256, 1668])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:18:04.799928Z",
     "end_time": "2023-04-12T16:18:04.803747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(1668, 256)\n",
    "\n",
    "x = linear(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:32:08.720488Z",
     "end_time": "2023-04-12T16:32:08.766758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 256, 256])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:32:08.925028Z",
     "end_time": "2023-04-12T16:32:08.969965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('recg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b793216b3dd7b47ffae26511feccbf1cb868298f39c032fb85acd5df274c591b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
